{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "tY9kuSJchB5I",
        "outputId": "009753a6-792f-4364-8c7b-bb74b30f570d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'CC GENERAL.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2119041180.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CC GENERAL.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'CC GENERAL.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"CC GENERAL.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "RCp24MwK7Rw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "D38YcsXQ7SZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df\n",
        "\n",
        "# dropped cust id as it is not a describing feature just an identifier for the\n",
        "# specific customer\n",
        "X = X.drop([\"CUST_ID\"], axis=1)\n",
        "\n",
        "# Its just one value lets just do median to get it\n",
        "X[\"CREDIT_LIMIT\"].fillna(X[\"CREDIT_LIMIT\"].median(), inplace=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "8GeXHUO97uYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 313 missing values (~3.5%)\n",
        "# Financial behavior feature\n",
        "# Missingness is not random\n",
        "# This means missingness itself carries information.\n",
        "X[\"MIN_PAY_MISSING\"] = X[\"MINIMUM_PAYMENTS\"].isna().astype(int)\n",
        "X[\"MINIMUM_PAYMENTS\"].fillna(X[\"MINIMUM_PAYMENTS\"].median(), inplace=True)\n",
        "\n",
        "\n",
        "print(X.isna().sum())\n",
        "print((X.isna().sum() / len(X)) * 100)\n"
      ],
      "metadata": {
        "id": "yKpBAMBi_Ml3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from itertools import combinations\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def generate_pca_report(n_components, df):\n",
        "    if n_components < 3:\n",
        "        raise ValueError(\"n_components must be >= 3 for 3D visualization\")\n",
        "\n",
        "    # ---- Preserve feature names\n",
        "    feature_names = df.columns\n",
        "\n",
        "    # ---- Scale\n",
        "    scaler = RobustScaler()\n",
        "    X_scaled = scaler.fit_transform(df)\n",
        "\n",
        "    # ---- PCA\n",
        "    pca = PCA(n_components=n_components)\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "    column_names = [f'PC{i+1}' for i in range(n_components)]\n",
        "    pca_df = pd.DataFrame(X_pca, columns=column_names)\n",
        "\n",
        "    # ---- 3D combinations\n",
        "    combos = list(combinations(column_names, 3))\n",
        "\n",
        "    fig = go.Figure()\n",
        "\n",
        "    for i, (pc1, pc2, pc3) in enumerate(combos):\n",
        "        x = pca_df[pc1].values\n",
        "        y = pca_df[pc2].values\n",
        "        z = pca_df[pc3].values\n",
        "\n",
        "        # ‚úÖ Distance from origin (depth)\n",
        "        distance = np.sqrt(x**2 + y**2 + z**2)\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Scatter3d(\n",
        "                x=x,\n",
        "                y=y,\n",
        "                z=z,\n",
        "                mode='markers',\n",
        "                marker=dict(\n",
        "                    size=2,\n",
        "                    color=distance,          # üëà DISTANCE-BASED COLOR\n",
        "                    colorscale='Turbo',\n",
        "                    opacity=0.75,\n",
        "                    showscale=True\n",
        "                ),\n",
        "                name=f'{pc1}-{pc2}-{pc3}',\n",
        "                visible=(i == 0)\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # ---- Dropdown for PC combinations\n",
        "    buttons = []\n",
        "    for i, (pc1, pc2, pc3) in enumerate(combos):\n",
        "        visible = [False] * len(combos)\n",
        "        visible[i] = True\n",
        "        buttons.append(\n",
        "            dict(\n",
        "                label=f'{pc1}-{pc2}-{pc3}',\n",
        "                method='update',\n",
        "                args=[\n",
        "                    {'visible': visible},\n",
        "                    {'scene': {\n",
        "                        'xaxis': {'title': pc1},\n",
        "                        'yaxis': {'title': pc2},\n",
        "                        'zaxis': {'title': pc3}\n",
        "                    }}\n",
        "                ]\n",
        "            )\n",
        "        )\n",
        "\n",
        "    fig.update_layout(\n",
        "        updatemenus=[dict(\n",
        "            buttons=buttons,\n",
        "            direction=\"down\",\n",
        "            x=0.02,\n",
        "            y=1.1\n",
        "        )],\n",
        "        title=\"Interactive 3D PCA (Color = Distance from Origin)\",\n",
        "        scene=dict(\n",
        "            xaxis_title=combos[0][0],\n",
        "            yaxis_title=combos[0][1],\n",
        "            zaxis_title=combos[0][2],\n",
        "        )\n",
        "    )\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "    # ---- PCA Loadings\n",
        "    loadings_df = pd.DataFrame(\n",
        "        pca.components_.T,\n",
        "        columns=column_names,\n",
        "        index=feature_names\n",
        "    )\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(\n",
        "        loadings_df,\n",
        "        cmap='RdBu_r',\n",
        "        center=0,\n",
        "        annot=True\n",
        "    )\n",
        "    plt.title(\"PCA Loading Scores\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # ---- Explained variance\n",
        "    print(\"\\nExplained Variance Ratio:\")\n",
        "    for i, var in enumerate(pca.explained_variance_ratio_, 1):\n",
        "        print(f\"PC{i}: {var:.4f} ({var*100:.2f}%)\")\n",
        "\n",
        "    print(f\"\\nCumulative Variance: {pca.explained_variance_ratio_.sum():.2%}\")\n",
        "    return pca_df\n"
      ],
      "metadata": {
        "id": "PBvQ_b4W-RI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_pca_scree_plot(X):\n",
        "  import matplotlib.pyplot as plt\n",
        "  from sklearn.decomposition import PCA\n",
        "\n",
        "  from sklearn.preprocessing import RobustScaler\n",
        "  scaler = RobustScaler()\n",
        "  X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "  # Fit PCA with all components\n",
        "  pca_full = PCA()\n",
        "  pca_full.fit(X_scaled)\n",
        "\n",
        "  explained_variance = pca_full.explained_variance_ratio_\n",
        "  cumulative_variance = explained_variance.cumsum()\n",
        "\n",
        "  # Scree (elbow) plot\n",
        "  plt.figure()\n",
        "  plt.plot(\n",
        "      range(1, len(explained_variance) + 1),\n",
        "      explained_variance,\n",
        "      marker='o'\n",
        "  )\n",
        "  plt.xlabel(\"Number of Principal Components\")\n",
        "  plt.ylabel(\"Explained Variance Ratio\")\n",
        "  plt.title(\"PCA Scree Plot (Elbow Method)\")\n",
        "  plt.show()\n",
        "\n",
        "  # Optional but useful: cumulative variance plot\n",
        "  plt.figure()\n",
        "  plt.plot(\n",
        "      range(1, len(cumulative_variance) + 1),\n",
        "      cumulative_variance,\n",
        "      marker='o'\n",
        "  )\n",
        "  plt.xlabel(\"Number of Principal Components\")\n",
        "  plt.ylabel(\"Cumulative Explained Variance\")\n",
        "  plt.title(\"Cumulative Explained Variance\")\n",
        "  plt.axhline(y=0.8, linestyle='--')  # common cutoff\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "q0NFpjK9-u_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_corr_graph_df(X):\n",
        "  import seaborn as sns\n",
        "\n",
        "  corr = X.corr()\n",
        "  plt.figure(figsize=(12,8))\n",
        "  sns.heatmap(corr, annot=True, fmt=\".2f\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "2RHAgVknDxsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "X.hist(bins=30, figsize=(20,15))\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "O52hzjzz_cVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check variance\n",
        "print(f\"TENURE variance: {X['TENURE'].var():.2f}\")\n",
        "print(f\"TENURE unique values: {X['TENURE'].nunique()}\")\n",
        "print(f\"% with TENURE=12: {(X['TENURE']==12).sum()/len(X)*100:.1f}%\")\n"
      ],
      "metadata": {
        "id": "cUP33KUSOV6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tenure is overloaded with 12 so it is not that useful because it does not give much information so lets remove it\n",
        "\n",
        "X = X.drop([\"TENURE\"], axis=1)"
      ],
      "metadata": {
        "id": "paHFjtZjOZft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in X.select_dtypes(include=[\"int64\", \"object\"]):\n",
        "    print(col, X[col].nunique())\n"
      ],
      "metadata": {
        "id": "b1oHNPWgSsHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_pca_scree_plot(X)"
      ],
      "metadata": {
        "id": "_iE4Nm9kcz3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Observation of the Raw Data\n",
        "\n",
        "Exploratory analysis of the dataset revealed that the majority of monetary variables (e.g., BALANCE, PURCHASES, CASH_ADVANCE, PAYMENTS, CREDIT_LIMIT) exhibited severely right-skewed distributions with long tails. Most customers had relatively small values, while a small subset showed extremely large magnitudes.\n",
        "\n",
        "Additionally, several frequency-based features were bounded between 0 and 1 and displayed heavy concentration at boundary values (particularly 0 and 1), indicating categorical-like behavioral patterns rather than continuous numerical variation.\n",
        "\n",
        "This distributional imbalance posed a significant problem for distance-based clustering algorithms.\n",
        "\n",
        "# Problem With Using Raw Features for Clustering\n",
        "\n",
        "Clustering algorithms such as K-Means rely on Euclidean distance, which is highly sensitive to feature scale and magnitude. In the raw data:\n",
        "\n",
        "Features with large numeric ranges dominated distance calculations\n",
        "\n",
        "Customers with extreme monetary values disproportionately influenced cluster centroidsProblem With Using Raw Features for Clustering\n",
        "\n",
        "Clustering algorithms such as K-Means rely on Euclidean distance, which is highly sensitive to feature scale and magnitude. In the raw data:\n",
        "\n",
        "- Features with large numeric ranges dominated distance calculations\n",
        "\n",
        "- Customers with extreme monetary values disproportionately influenced cluster centroids\n",
        "\n",
        "- Customers with moderate or low spending behavior collapsed into indistinguishable groups\n",
        "\n",
        "As a result, clustering on the raw data would primarily separate customers by spending volume, rather than by meaningful behavioral patterns.\n",
        "\n",
        "Customers with moderate or low spending behavior collapsed into indistinguishable groups\n",
        "\n",
        "As a result, clustering on the raw data would primarily separate customers by spending volume, rather than by meaningful behavioral patterns.\n",
        "\n",
        "# Log Transformation of Monetary Features\n",
        "\n",
        "To address the extreme skewness and reduce the influence of outliers, a logarithmic transformation was applied to monetary features using the log1p function.\n",
        "\n",
        "Rationale:\n",
        "\n",
        "- Compresses long right tails while preserving relative ordering\n",
        "\n",
        "- Reduces dominance of extreme values without discarding data\n",
        "\n",
        "- Stabilizes variance across customers\n",
        "\n",
        "Observed Effect:\n",
        "After log transformation, previously heavy-tailed distributions became more symmetric and spread more evenly across their range. This improved the ability of distance-based methods to disti"
      ],
      "metadata": {
        "id": "W7qIxp00A4nc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_cols = [\n",
        "    \"BALANCE\",\n",
        "    \"PAYMENTS\",\n",
        "    \"MINIMUM_PAYMENTS\",\n",
        "    \"CREDIT_LIMIT\",\n",
        "]\n",
        "\n",
        "import numpy as np\n",
        "X[log_cols] = X[log_cols].apply(np.log1p)\n",
        "\n",
        "# check for negative values\n",
        "(X[log_cols] < 0).sum()\n"
      ],
      "metadata": {
        "id": "SCjgk_sK-FTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZJXVWtlf-0dR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.hist(bins=30, figsize=(20,15))\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Jnv-wryo-bCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_pca_scree_plot(X)"
      ],
      "metadata": {
        "id": "tGHO01N9bfro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After doing log transformation on all features we found better"
      ],
      "metadata": {
        "id": "LIyz3jRXNk28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing Justification: Handling Zero-Inflated Cash Advance Features\n",
        "\n",
        "## Problem Identified\n",
        "The cash advance features (CASH_ADVANCE, CASH_ADVANCE_FREQUENCY, CASH_ADVANCE_TRX) exhibit severe zero-inflation, with approximately 75% of customers showing zero values. This bimodal distribution (zero vs. non-zero) causes two critical issues for clustering:\n",
        "\n",
        "1. **Distance metric distortion**: Euclidean distance calculations become dominated by the binary pattern of \"uses vs. doesn't use\" rather than capturing nuanced behavioral differences\n",
        "2. **Feature redundancy**: Three variables measure overlapping aspects of the same behavior (cash advance usage), leading to multicollinearity and over-weighting this single dimension\n",
        "\n",
        "## Solution Applied\n",
        "We engineer three complementary features that capture distinct aspects of cash advance behavior:\n",
        "\n",
        "### 1. Binary Indicator (`uses_cash_advance`)\n",
        "```python\n",
        "uses_cash_advance = (CASH_ADVANCE > 0).astype(int)\n",
        "```\n",
        "- Captures the primary behavioral split: cash advance users vs. non-users\n",
        "- Provides clear cluster interpretability\n",
        "\n",
        "### 2. Economic Magnitude (`cash_advance_amount_log`)\n",
        "```python\n",
        "cash_advance_amount_log = log1p(CASH_ADVANCE)\n",
        "```\n",
        "- Captures the monetary value of cash advance usage\n",
        "- Log transformation normalizes the right-skewed distribution\n",
        "- Preserves zero values (log1p(0) = 0)\n",
        "\n",
        "### 3. Behavioral Pattern (`cash_advance_frequency`)\n",
        "```python\n",
        "cash_advance_frequency = CASH_ADVANCE_FREQUENCY  # Retained as-is\n",
        "```\n",
        "- Captures usage frequency (already normalized 0-1)\n",
        "- Distinguishes one-time large withdrawals from frequent small withdrawals\n",
        "- Reveals chronic vs. emergency usage patterns\n",
        "\n",
        "### Features Dropped\n",
        "- **CASH_ADVANCE** ‚Üí Replaced by log-transformed version\n",
        "- **CASH_ADVANCE_TRX** ‚Üí Redundant given amount and frequency\n"
      ],
      "metadata": {
        "id": "hn2oiFO3RSM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X['uses_cash_advance'] = (X['CASH_ADVANCE'] > 0).astype(int)\n",
        "X['cash_advance_log'] = np.log1p(X['CASH_ADVANCE'])\n",
        "X['cash_advance_frequency'] = X['CASH_ADVANCE_FREQUENCY']\n",
        "X = X.drop(['CASH_ADVANCE_FREQUENCY', 'CASH_ADVANCE_TRX', 'CASH_ADVANCE'], axis=1)\n",
        "X[['uses_cash_advance', 'cash_advance_log', 'cash_advance_frequency']].describe()"
      ],
      "metadata": {
        "id": "W-fLz42gFfpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aWNDEDm2nGrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[['uses_cash_advance', 'cash_advance_log', 'cash_advance_frequency']].hist(bins=30)"
      ],
      "metadata": {
        "id": "ns-M1aLnOztz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Cash Advance Usage Distribution:\")\n",
        "print(X['uses_cash_advance'].value_counts().sort_index())\n",
        "print()\n",
        "\n",
        "# With percentages\n",
        "print(\"Cash Advance Usage Distribution (with percentages):\")\n",
        "print(X['uses_cash_advance'].value_counts(normalize=True).sort_index() * 100)\n",
        "print()\n",
        "\n",
        "# More detailed breakdown\n",
        "print(\"\\nDetailed Summary:\")\n",
        "print(f\"Non-users (0): {(X['uses_cash_advance'] == 0).sum()} customers ({(X['uses_cash_advance'] == 0).sum() / len(X) * 100:.1f}%)\")\n",
        "print(f\"Users (1): {(X['uses_cash_advance'] == 1).sum()} customers ({(X['uses_cash_advance'] == 1).sum() / len(X) * 100:.1f}%)\")\n",
        "print(f\"Total: {len(X)} customers\")"
      ],
      "metadata": {
        "id": "cx7MyqHbSr50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Results of Cash Advance Feature Engineering\n",
        "\n",
        "### Transformed Distributions\n",
        "\n",
        "**`uses_cash_advance` (Binary Indicator)**\n",
        "- Clean binary split: 4,628 non-users (0) vs. 4,322 users (1)\n",
        "- 48.3% of customers use cash advances\n",
        "\n",
        "**`cash_advance_amount_log` (Log-Transformed Amount)**\n",
        "- Preserves zero spike for non-users (~4,628)\n",
        "- Users show smooth distribution from 2.5 to 10.0\n",
        "- Successfully normalizes extreme values while maintaining economic magnitude distinctions\n",
        "\n",
        "**`cash_advance_frequency` (Retained)**\n",
        "- Ranges from 0 (non-users) to 1 (use in every billing cycle)\n",
        "- Among users: distinguishes sporadic vs. frequent usage patterns\n",
        "\n",
        "## Key Improvements\n",
        "\n",
        "1. **Eliminated redundancy**: Three correlated features ‚Üí three orthogonal dimensions\n",
        "2. **Balanced information**: Binary flag (user type) + continuous amount (magnitude) + frequency (pattern)\n",
        "3. **Enhanced separability**: Non-users form distinct group; users differentiate by both economic impact and behavioral frequency\n",
        "4. **Prevents zero-inflation dominance**: Clustering can now capture nuanced behavior beyond simple \"uses vs. doesn't use\"\n",
        "5. **Preserves behavioral nuance**: Frequency enables distinction between emergency borrowers (high amount, low frequency) and chronic users (moderate amount, high frequency)\n"
      ],
      "metadata": {
        "id": "TXmogngASB-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# same as above\n",
        "\n",
        "X['uses_installments'] = (X['INSTALLMENTS_PURCHASES'] > 0).astype(int)\n",
        "X['installments_amount_log'] = np.log1p(X['INSTALLMENTS_PURCHASES'])\n",
        "X = X.drop(['INSTALLMENTS_PURCHASES'], axis=1)"
      ],
      "metadata": {
        "id": "T0uK6kzRO3NG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X['uses_oneoff'] = (X['ONEOFF_PURCHASES'] > 0).astype(int)\n",
        "X['oneoff_amount_log'] = np.log1p(X['ONEOFF_PURCHASES'])\n",
        "X = X.drop(['ONEOFF_PURCHASES'], axis=1)"
      ],
      "metadata": {
        "id": "ZShX8BRwkBIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep balance frequency as its an indicator of activity"
      ],
      "metadata": {
        "id": "W_vlMpbOkOhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Purchase frequency captures important behaviors for non buyers vs frequent buyers"
      ],
      "metadata": {
        "id": "tWse1vZRkSf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[\"uses_purchase\"] = (X['PURCHASES'] > 0).astype(int)\n",
        "X['purchases_log'] = np.log1p(X['PURCHASES'])\n",
        "X['purchase_frequency'] = X['PURCHASES_FREQUENCY']\n",
        "X['purchases_trx_log'] = np.log1p(X['PURCHASES_TRX'])\n",
        "X = X.drop(['PURCHASES', \"PURCHASES_FREQUENCY\", 'PURCHASES_TRX'], axis=1)"
      ],
      "metadata": {
        "id": "1J55EOvikdxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_pca_scree_plot(X)"
      ],
      "metadata": {
        "id": "O-mJwhLBbc10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.hist(bins=30, figsize=(20,15))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tc0Dd0h1l7_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_corr_graph_df(X)"
      ],
      "metadata": {
        "id": "i1bxbjCuUNXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I removed highly correlated behavioral proxies to avoid overweighting the same customer actions multiple times in distance-based clustering."
      ],
      "metadata": {
        "id": "_Z3dsQGTZ0oe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = X.drop([\"uses_cash_advance\",\n",
        "            \"cash_advance_frequency\",\n",
        "            \"uses_installments\",\n",
        "            \"uses_oneoff\",\n",
        "            \"purchases_trx_log\",\n",
        "            \"uses_purchase\",\n",
        "            \"PURCHASES_INSTALLMENTS_FREQUENCY\",\n",
        "            ], axis=1)"
      ],
      "metadata": {
        "id": "djerUP_mZ3N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.hist(bins=30, figsize=(20,15))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eT11ZlmRPblI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_corr_graph_df(X)"
      ],
      "metadata": {
        "id": "SYh9H9swaZPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_pca_scree_plot(X)"
      ],
      "metadata": {
        "id": "yjsDpaHnl8dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_pca_report(6, X)"
      ],
      "metadata": {
        "id": "cJI7esUt7Y-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca_df = generate_pca_report(4, X)"
      ],
      "metadata": {
        "id": "ImFpv7pW-Z7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Principal Component Analysis (PCA) Interpretation and Component Selection\n",
        "\n",
        "We performed Principal Component Analysis (PCA) using two configurations: **4 principal components (PCs)** and **6 principal components**, explaining **88.28%** and **95.23%** of the total variance respectively. This section provides a detailed interpretation of the extracted components and justifies the selection of **4 PCs for downstream analysis**.\n",
        "\n",
        "---\n",
        "\n",
        "## Interpretation of Principal Components (4-PC Solution)\n",
        "\n",
        "### PC1 ‚Äì Credit Utilization & Balance Behavior\n",
        "**Key loadings**\n",
        "- BALANCE_FREQUENCY (‚âà +0.81)\n",
        "- BALANCE (‚âà +0.26)\n",
        "- MINIMUM_PAYMENTS (‚âà +0.20)\n",
        "- PRC_FULL_PAYMENT (‚âà ‚àí0.45)\n",
        "\n",
        "**Explanation**  \n",
        "PC1 captures how frequently customers carry outstanding balances versus paying in full. High PC1 scores correspond to customers who regularly revolve balances and rely on minimum payments, while low scores indicate disciplined users who consistently pay in full. This component represents the primary axis of credit usage intensity.\n",
        "\n",
        "---\n",
        "\n",
        "### PC2 ‚Äì Payment Discipline\n",
        "**Key loadings**\n",
        "- PRC_FULL_PAYMENT (‚âà +0.82)\n",
        "- BALANCE_FREQUENCY (‚âà +0.42)\n",
        "\n",
        "**Explanation**  \n",
        "PC2 reflects repayment reliability independent of spending volume. Customers with high scores consistently pay their balances in full, whereas lower scores indicate partial or inconsistent repayment behavior. This component isolates payment discipline as a distinct behavioral dimension.\n",
        "\n",
        "---\n",
        "\n",
        "### PC3 ‚Äì Spending Structure and Purchase Behavior\n",
        "**Key loadings**\n",
        "- ONEOFF_PURCHASES_FREQUENCY (‚âà +0.59)\n",
        "- purchases_log (‚âà +0.52)\n",
        "- oneoff_amount_log (‚âà +0.32)\n",
        "- purchase_frequency (‚âà +0.20)\n",
        "\n",
        "**Explanation**  \n",
        "PC3 represents how customers spend, distinguishing between frequent purchases and one-off, higher-value transactions. This component captures consumption and transaction patterns rather than repayment or balance management behavior.\n",
        "\n",
        "---\n",
        "\n",
        "### PC4 ‚Äì Repayment Pressure and Liquidity Stress\n",
        "**Key loadings**\n",
        "- PAYMENTS (‚âà +0.72)\n",
        "- MINIMUM_PAYMENTS (‚âà +0.31)\n",
        "- BALANCE (‚âà +0.29)\n",
        "- cash_advance_log (‚âà +0.26)\n",
        "\n",
        "**Explanation**  \n",
        "PC4 reflects financial strain and liquidity pressure. Higher scores indicate customers making larger payments, holding higher balances, and relying more on cash advances, suggesting increased repayment burden or short-term liquidity needs.\n",
        "\n",
        "---\n",
        "\n",
        "## Interpretation of Additional Components (6-PC Solution)\n",
        "\n",
        "Extending the PCA to **6 components** increases the explained variance to **95.23%**, but the additional components provide limited new behavioral insight.\n",
        "\n",
        "---\n",
        "\n",
        "### PC5 ‚Äì Installment vs One-Off Spending Contrast\n",
        "**Key loadings**\n",
        "- installments_amount_log (‚âà ‚àí0.42)\n",
        "- purchases_log (‚âà ‚àí0.42)\n",
        "- ONEOFF_PURCHASES_FREQUENCY (‚âà +0.63)\n",
        "\n",
        "**Explanation**  \n",
        "PC5 contrasts installment-heavy spending with direct or one-off purchases. While this component adds granularity to spending behavior, it does not introduce a fundamentally new behavioral dimension beyond what is already captured by PC3.\n",
        "\n",
        "---\n",
        "\n",
        "### PC6 ‚Äì Minimum Payment Dominance\n",
        "**Key loadings**\n",
        "- MINIMUM_PAYMENTS (‚âà +0.72)\n",
        "- Secondary contributions from PAYMENTS and CREDIT_LIMIT\n",
        "\n",
        "**Explanation**  \n",
        "PC6 is largely driven by a single variable, indicating that the PCA is capturing residual variance rather than meaningful latent structure. Components dominated by a single feature are generally unstable and offer limited interpretive value.\n",
        "\n",
        "---\n",
        "\n",
        "## Justification for Selecting 4 PCs\n",
        "\n",
        "### Variance Coverage\n",
        "- 4 PCs explain **88.28%** of total variance\n",
        "- 6 PCs explain **95.23%**, adding only ~7% additional variance\n",
        "\n",
        "The majority of meaningful structure is already captured within the first four components.\n",
        "\n",
        "---\n",
        "\n",
        "### Interpretability and Behavioral Coverage\n",
        "The 4-PC solution cleanly represents four distinct and interpretable behavioral dimensions:\n",
        "1. Credit utilization intensity  \n",
        "2. Payment discipline  \n",
        "3. Spending patterns  \n",
        "4. Financial strain  \n",
        "\n",
        "Additional components mainly refine existing patterns rather than reveal new structure.\n",
        "\n",
        "---\n",
        "\n",
        "### Model Simplicity and Stability\n",
        "Using fewer components reduces dimensionality, limits noise, and improves model stability and generalization in downstream tasks such as clustering or predictive modeling.\n",
        "\n",
        "---\n",
        "\n",
        "## Final Justification Statement\n",
        "\n",
        "Although the 6-component PCA explains a higher proportion of total variance, the additional components primarily capture marginal or redundant patterns. The 4-component solution, explaining 88.28% of the variance, retains the key behavioral dimensions of credit usage, payment discipline, spending behavior, and financial strain, providing a more interpretable and robust representation for downstream analysis.\n"
      ],
      "metadata": {
        "id": "EnF_2nbwtoQ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scaling of Features Using RobustScaler\n",
        "\n",
        "After log transformation, features were scaled using RobustScaler, which normalizes data based on the median and interquartile range.\n",
        "\n",
        "Rationale:\n",
        "\n",
        "- Ensures all features contribute comparably to distance calculations\n",
        "\n",
        "- Prevents residual outliers from disproportionately influencing clusters\n",
        "\n",
        "- More appropriate than standard scaling for non-Gaussian data\n",
        "\n",
        "Observed Effect:\n",
        "Scaling aligned features onto a comparable numeric range without reintroducing sensitivity to extreme values, allowing clustering algorithms to weigh behavioral and monetary features more evenly."
      ],
      "metadata": {
        "id": "QdzyeNGFBNYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pc_names = {\n",
        "    \"PC1\": \"Credit Utilization Intensity\",\n",
        "    \"PC2\": \"Payment Discipline\",\n",
        "    \"PC3\": \"Spending Pattern\",\n",
        "    \"PC4\": \"Financial Strain\"\n",
        "}\n",
        "\n",
        "pca_df = pca_df.rename(columns=pc_names)\n",
        "pca_df"
      ],
      "metadata": {
        "id": "6SxGZBSyEr5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering Method Selection and Justification\n",
        "\n",
        "## Objective\n",
        "The goal of this analysis is to identify meaningful patterns in **credit card utilization behavior** by clustering individuals based on latent behavioral dimensions derived from PCA. These clusters are intended to represent distinct financial behavior profiles rather than artificially separated geometric groupings.\n",
        "\n",
        "---\n",
        "\n",
        "## Feature Representation\n",
        "\n",
        "After preprocessing and dimensionality reduction, the data is represented using four principal components:\n",
        "\n",
        "- **PC1 ‚Äì Credit Utilization Intensity**  \n",
        "- **PC2 ‚Äì Payment Discipline**  \n",
        "- **PC3 ‚Äì Spending Pattern**  \n",
        "- **PC4 ‚Äì Financial Strain**\n",
        "\n",
        "These components capture the dominant behavioral axes of credit card usage and are used as inputs to the clustering algorithms.\n",
        "\n",
        "---\n",
        "\n",
        "## Observations After PCA\n",
        "\n",
        "Visual inspection of the PCA-transformed space shows:\n",
        "- No clear spherical or compact clusters\n",
        "- Points are broadly distributed, forming a near-uniform or ‚Äúcube-like‚Äù structure\n",
        "- Significant overlap between behavioral patterns is expected, especially for users with moderate credit behavior\n",
        "\n",
        "These observations strongly influence algorithm selection.\n",
        "\n",
        "---\n",
        "\n",
        "## Considered Clustering Algorithms\n",
        "\n",
        "### 1. K-Means Clustering (Not Selected as Primary)\n",
        "\n",
        "**Rationale for consideration**:\n",
        "- Simple and widely used\n",
        "- Operates based on distances between points\n",
        "- Can segment users based on relative position in feature space\n",
        "\n",
        "**Limitations in this context**:\n",
        "- Assumes spherical, equally sized clusters\n",
        "- Requires pre-specifying the number of clusters\n",
        "- Sensitive to outliers\n",
        "- Poor fit for overlapping and irregular behavioral patterns\n",
        "\n",
        "**Conclusion**:  \n",
        "K-means was explored during the exploratory phase but was **not selected as the primary method** due to the absence of spherical cluster structure and the presence of overlapping behaviors. Its assumptions do not align well with the observed data geometry.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Hierarchical Clustering (Support\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Hierarchical Clustering (Supporting Analysis)\n",
        "\n",
        "**Why it is suitable**:\n",
        "- Does not require specifying the number of clusters upfront\n",
        "- Captures hierarchical relationships between observations\n",
        "- Useful for analyzing how behavioral patterns split at different similarity levels\n",
        "\n",
        "**Relevance to this study**:\n",
        "- PCA components represent related financial behaviors\n",
        "- Hierarchical clustering allows inspection of nested groupings (e.g., disciplined vs. undisciplined users, then further subgroups)\n",
        "\n",
        "**Role in the analysis**:\n",
        "- Used to **explore structure and validate relationships**\n",
        "- Dendrograms aid interpretability\n",
        "- Not used as the final clustering method due to scalability and sensitivity to linkage choices\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Density-Based Clustering (DBSCAN / HDBSCAN)\n",
        "\n",
        "**Why this is appropriate**:\n",
        "- Does not assume spherical cluster shapes\n",
        "- Can identify clusters of arbitrary geometry\n",
        "- Explicitly models noise and outliers\n",
        "- Does not require pre-defining the number of clusters\n",
        "\n",
        "**Relevance to credit behavior data**:\n",
        "- Financial behaviors often form dense regions with gradual transitions\n",
        "- Users with extreme utilization or financial strain naturally appear as outliers\n",
        "- The cube-like distribution suggests density variation rather than clear centroids\n",
        "\n",
        "**Conclusion**:  \n",
        "Density-based clustering is well-suited for uncovering **irregular, behavior-driven groupings** and identifying atypical users.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Gaussian Mixture Models (GMM)\n",
        "\n",
        "**Why this is appropriate**:\n",
        "- Models clusters as probability distributions\n",
        "- Allows **overlapping clusters**\n",
        "- Provides soft assignments (membership probabilities)\n",
        "\n",
        "**Relevance to this study**:\n",
        "- Credit behaviors are not strictly separable\n",
        "- Users in transitional financial states (e.g., improving or deteriorating discipline) are expected\n",
        "- GMM captures uncertainty in cluster membership\n",
        "\n",
        "**Conclusion**:  \n",
        "GMM is particularly suitable for representing **continuous and overlapping financial behavior segments**, which aligns with real-world credit usage patterns.\n",
        "\n",
        "---\n",
        "\n",
        "## Final Methodological Choice\n",
        "\n",
        "- **Primary methods**:  \n",
        "  - Density-Based Clustering (DBSCAN or HDBSCAN)  \n",
        "  - Gaussian Mixture Models (GMM)\n",
        "\n",
        "- **Supporting analysis**:  \n",
        "  - Hierarchical clustering for structural validation\n",
        "\n",
        "- **Exploratory only**:  \n",
        "  - K-means (not used for final segmentation)\n",
        "\n",
        "This combination balances:\n",
        "- Flexibility in cluster shape\n",
        "- Ability to model overlapK-means was explored during the exploratory phase but was not selected as the primary method due to the absence of spherical cluster structure and the presence of overlapping behaviors. Its assumptions do not align well with the observed data geometry.\n",
        "- Interpretability\n",
        "- Alignment with domain expectations\n",
        "\n",
        "---\n",
        "\n",
        "## Cluster Validation and Interpretation\n",
        "\n",
        "Clusters are evaluated using:\n",
        "- Internal validation metrics (e.g., silhouette score, Davies‚ÄìBouldin index)\n",
        "- Stability analysis across random seeds and subsampling\n",
        "- Behavioral interpretability using feature distributions per cluster\n",
        "\n",
        "Each resulting cluster is characterized in terms of:\n",
        "- Credit utilization intensity\n",
        "- Payment discipline\n",
        "- Spending behavior\n",
        "- Financial strain profile\n",
        "\n",
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "The selected clustering approaches reflect both the **statistical structure of the data** and the **real-world complexity of financial behavior**. Rather than enforcing artificial separation, the methodology prioritizes interpretability, robustness, and domain relevance.\n"
      ],
      "metadata": {
        "id": "XGnw8keLyRxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "number_of_features = 4\n",
        "min_samples = 2 * number_of_features\n",
        "\n",
        "\n",
        "k = min_samples\n",
        "\n",
        "nn = NearestNeighbors(n_neighbors=k)\n",
        "nn.fit(pca_df)\n",
        "\n",
        "distances, _ = nn.kneighbors(pca_df)\n",
        "k_distances = np.sort(distances[:, k-1])\n",
        "\n",
        "plt.plot(k_distances)\n",
        "plt.ylabel(f\"{k}-NN distance\")\n",
        "plt.xlabel(\"Points sorted by distance\")\n",
        "plt.show()\n",
        "\n",
        "y = k_distances\n",
        "x = np.arange(len(y))\n",
        "p1 = np.array([x[0], y[0]])\n",
        "p2 = np.array([x[-1], y[-1]])\n",
        "\n",
        "distances = np.abs(\n",
        "    np.cross(p2 - p1, p1 - np.vstack((x, y)).T)\n",
        ") / np.linalg.norm(p2 - p1)\n",
        "\n",
        "elbow_index = np.argmax(distances)\n",
        "optimal_eps = y[elbow_index]\n",
        "\n",
        "print(\"Min Samples based on dimentionality rule of thumb:\", min_samples)\n",
        "print(\"Optimal eps:\", optimal_eps)\n"
      ],
      "metadata": {
        "id": "Te2LKDI3uhOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "optimal_eps=0.3\n",
        "min_smples=8\n",
        "db = DBSCAN(eps=optimal_eps, min_samples=min_samples, metric=\"euclidean\")\n",
        "db.fit(pca_df)\n",
        "labels = db.labels_\n",
        "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "print(f\"clusters={n_clusters}\")"
      ],
      "metadata": {
        "id": "ngxaQiAf3wsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_3d_cluster_report(df, labels):\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import plotly.graph_objects as go\n",
        "    from itertools import combinations\n",
        "\n",
        "    # ---- Validation\n",
        "    if df.shape[1] < 3:\n",
        "        raise ValueError(\"Dataset must have at least 3 features\")\n",
        "\n",
        "    if len(df) != len(labels):\n",
        "        raise ValueError(\"df and labels must have the same length\")\n",
        "\n",
        "    # ---- Ensure labels are 1D\n",
        "    if isinstance(labels, pd.DataFrame):\n",
        "        labels = labels.iloc[:, 0]\n",
        "    labels = np.asarray(labels)\n",
        "\n",
        "    feature_names = df.columns\n",
        "    combos = list(combinations(feature_names, 3))\n",
        "\n",
        "    fig = go.Figure()\n",
        "\n",
        "    for i, (f1, f2, f3) in enumerate(combos):\n",
        "        fig.add_trace(\n",
        "            go.Scatter3d(\n",
        "                x=df[f1],\n",
        "                y=df[f2],\n",
        "                z=df[f3],\n",
        "                mode='markers',\n",
        "                marker=dict(\n",
        "                    size=3,\n",
        "                    color=labels,          # üëà CLUSTER LABELS\n",
        "                    colorscale='Turbo',\n",
        "                    opacity=0.8,\n",
        "                    showscale=True,\n",
        "                    colorbar=dict(title=\"Cluster\")\n",
        "                ),\n",
        "                name=f\"{f1}-{f2}-{f3}\",\n",
        "                visible=(i == 0)\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # ---- Dropdown\n",
        "    buttons = []\n",
        "    for i, (f1, f2, f3) in enumerate(combos):\n",
        "        visible = [False] * len(combos)\n",
        "        visible[i] = True\n",
        "        buttons.append(\n",
        "            dict(\n",
        "                label=f\"{f1}-{f2}-{f3}\",\n",
        "                method=\"update\",\n",
        "                args=[\n",
        "                    {\"visible\": visible},\n",
        "                    {\"scene\": {\n",
        "                        \"xaxis\": {\"title\": f1},\n",
        "                        \"yaxis\": {\"title\": f2},\n",
        "                        \"zaxis\": {\"title\": f3}\n",
        "                    }}\n",
        "                ]\n",
        "            )\n",
        "        )\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=\"Interactive 3D Feature Space (Colored by Cluster Labels)\",\n",
        "        updatemenus=[dict(\n",
        "            buttons=buttons,\n",
        "            direction=\"down\",\n",
        "            x=1,\n",
        "            y=2\n",
        "        )],\n",
        "        scene=dict(\n",
        "            xaxis_title=combos[0][0],\n",
        "            yaxis_title=combos[0][1],\n",
        "            zaxis_title=combos[0][2]\n",
        "        )\n",
        "    )\n",
        "\n",
        "    fig.show()\n"
      ],
      "metadata": {
        "id": "Xrlnphhy6L0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_3d_cluster_report(df=pca_df, labels=labels)"
      ],
      "metadata": {
        "id": "Bf7W7H2K5BWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(labels).value_counts()\n"
      ],
      "metadata": {
        "id": "SgbUwGj85PLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hb-Bjktr7mhC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}